{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annalina-Luo/ClipNews/blob/main/ClipNews_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Process notenook for [ClipNews]((https://github.com/Annalina-Luo/NewsClip.git))"
      ],
      "metadata": {
        "id": "GA8ouqmEU3XA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-vtDmN2s4eb",
        "outputId": "f1f2e812-5fe5-4b67-9d43-6d9040c8593e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ClipNews'...\n",
            "remote: Enumerating objects: 224, done.\u001b[K\n",
            "remote: Counting objects: 100% (224/224), done.\u001b[K\n",
            "remote: Compressing objects: 100% (174/174), done.\u001b[K\n",
            "remote: Total 224 (delta 117), reused 119 (delta 42), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (224/224), 19.62 MiB | 28.41 MiB/s, done.\n",
            "Resolving deltas: 100% (117/117), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-os9eiqnt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-os9eiqnt\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (0.15.1+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.11.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369398 sha256=d7323b3ec40ebbe88adbdab89cecc5d6b0f9f419d86a4c032353e6d3b1f0f267\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-exl_zg1i/wheels/c8/e4/e1/11374c111387672fc2068dfbe0d4b424cb9cdd1b2e184a71b5\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n"
          ]
        }
      ],
      "source": [
        "#@title Install\n",
        "# you can upload the codes or clone them from github.\n",
        "!git clone https://github.com/Annalina-Luo/ClipNews.git\n",
        "!pip install transformers\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ClipNews/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKLU6CdhaK8F",
        "outputId": "bbb369ea-d468-4611-be16-899c073a48bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ClipNews\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the training data and checkpoints for previous models\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "class Downloader(object):\n",
        "    def __init__(self):\n",
        "        self.authenticate()\n",
        "        \n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "    \n",
        "    def download_file(self, file_id, file_dst):\n",
        "        downloaded = self.drive.CreateFile({'id':file_id})\n",
        "        downloaded.FetchMetadata(fetch_all=True)\n",
        "        downloaded.GetContentFile(file_dst)\n",
        "\n",
        "downloader = Downloader()"
      ],
      "metadata": {
        "id": "t4m-PFRo7bDU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it will take a long time to download our data\n",
        "downloader.download_file(\"1XQ1iAFs91jOtP6DiklAxnd3Mq7ZPGIrx\", \"images_processed.tar.gz\")\n",
        "downloader.download_file(\"1MG8svnIq_h8_WpNOcCVM86t_sKGrsykR\", \"train.json\")\n",
        "downloader.download_file(\"1h8G_dOyK6eMEFmTIkX4ofdvDV1GjZ2nv\", \"val.json\")\n",
        "downloader.download_file(\"1H22CjcLBJI5UB1zsw6j1jRTkouq11Rds\", \"val_gts.json\")\n",
        "downloader.download_file(\"1pzuxjyMmvFPS_qk2Lfro-PkDbC6gTqj5\", \"./checkpoint/checkpoint_ClipNews.pth.tar\")"
      ],
      "metadata": {
        "id": "4tTTjCPibD7s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it will take a long time to unzip the image files\n",
        "!tar -zxf ./images_processed.tar.gz"
      ],
      "metadata": {
        "id": "biAo3qXxbFmD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0-CDG12wPch",
        "outputId": "c73413ab-f528-4051-8d40-db87d647ded2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rTrain 28:   0% 0/170 [00:00<?, ?it/s]/content/NewsClip/model.py:193: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  out = torch.tensor(out, dtype=torch.float32)\n",
            "/content/NewsClip/model.py:392: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  index1 = torch.tensor(index1, dtype=torch.int64)\n",
            "Train 28: 100% 170/170 [1:13:37<00:00, 25.99s/it]\n",
            "Epoch [28/150], Loss: 0.0636, Perplexity: 1.0657\n",
            "Train 29: 100% 170/170 [1:12:50<00:00, 25.71s/it]\n",
            "Epoch [29/150], Loss: 0.0275, Perplexity: 1.0279\n",
            "Dev 29:   0% 0/10870 [00:00<?, ?it/s]2023-04-12 20:00:37.176844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Dev 29:   0% 1/10870 [00:08<26:53:19,  8.91s/it]/content/NewsClip/model.py:193: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  out = torch.tensor(out, dtype=torch.float32)\n",
            "/content/NewsClip/model.py:392: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  index1 = torch.tensor(index1, dtype=torch.int64)\n",
            "Dev 29: 100% 10870/10870 [1:39:52<00:00,  1.81it/s]\n",
            "Epoch [29/150], Loss: 0.0116, Perplexity: 1.0117\n",
            "best_cider: 0.0012307565546354872\n",
            "learning_rate: 0.0005\n",
            "\n",
            "Epoch since last improvement: 1\n",
            "\n",
            "Train 30: 100% 170/170 [1:12:38<00:00, 25.64s/it]\n",
            "Epoch [30/150], Loss: 0.0270, Perplexity: 1.0273\n",
            "Dev 30: 100% 10870/10870 [1:38:18<00:00,  1.84it/s]\n",
            "Epoch [30/150], Loss: 0.0126, Perplexity: 1.0126\n",
            "best_cider: 0.0012307565546354872\n",
            "learning_rate: 0.0005\n",
            "\n",
            "Epoch since last improvement: 2\n",
            "\n",
            "Train 31: 100% 170/170 [1:12:34<00:00, 25.62s/it]\n",
            "Epoch [31/150], Loss: 0.0276, Perplexity: 1.0280\n",
            "Dev 31:  24% 2596/10870 [23:50<1:21:20,  1.70it/s]Exception ignored in: <generator object tqdm.__iter__ at 0x7f8a4fd44270>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1193, in __iter__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1299, in close\n",
            "    self.display(pos=0)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1492, in display\n",
            "    self.sp(self.__str__() if msg is None else msg)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1148, in __str__\n",
            "    return self.format_meter(**self.format_dict)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 434, in format_meter\n",
            "    elapsed_str = tqdm.format_interval(elapsed)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 305, in format_interval\n",
            "    return '{0:02d}:{1:02d}'.format(m, s)\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/NewsClip/main.py\", line 307, in <module>\n",
            "    main(args)\n",
            "  File \"/content/NewsClip/main.py\", line 153, in main\n",
            "    recent_cider = validate(model=model,\n",
            "  File \"/content/NewsClip/main.py\", line 276, in validate\n",
            "    prediction = translate_sentence(\n",
            "  File \"/content/NewsClip/model.py\", line 563, in translate_sentence\n",
            "    output = model.decoder(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/NewsClip/model.py\", line 374, in forward\n",
            "    trg, trg_src, trg_image, attention_src = layer(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/NewsClip/model.py\", line 452, in forward\n",
            "    _trg0, attention_src = self.encoder_attention(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/NewsClip/model.py\", line 137, in forward\n",
            "    x2 = self.l2(x)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#@title Training Process\n",
        "# change the path of annotation files using `--ann_path`\n",
        "# change the path of gts file files using `--gts_file_dev`\n",
        "# change the attention module in the architecture `--TextEncoder_attention=True` or `--ImageEncoder_attention=True`\n",
        "# you can continue our last training using `--checkpoint=\"./checkpoint/checkpoint_ClipNews.pth.tar\"` (using GPU)\n",
        "!python main.py --num_workers=0 --image_dir=\"./images_processed/\" --batch_size=64 --TextEncoder_attention=True"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wfVjoEpxfO6p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QxQt1-fFF7VijTa3QECbfK9DjHnePHE4",
      "authorship_tag": "ABX9TyN+A/NNEi/vCssytuQ9KnqP",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}