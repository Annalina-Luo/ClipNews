{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annalina-Luo/NewsClip/blob/main/Dict/NewsClip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-vtDmN2s4eb",
        "outputId": "44532557-04d1-4858-c0ce-f998104bb8ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NewsClip'...\n",
            "remote: Enumerating objects: 123, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 123 (delta 55), reused 74 (delta 21), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (123/123), 19.59 MiB | 8.99 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Annalina-Luo/NewsClip.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqt2eCQM1hW7",
        "outputId": "612619b8-3962-4ac5-d78c-68b2ae132ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NewsClip\n"
          ]
        }
      ],
      "source": [
        "cd NewsClip/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3PRU6KTv3qB"
      },
      "outputs": [],
      "source": [
        "!tar -zxf ../drive/MyDrive/Colab_Notebooks/images_processed.tar.gz "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFKXSoXh8X4f",
        "outputId": "db9d2926-7f64-4267-9a90-ce3762f664c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-_h848ox1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-_h848ox1\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (0.14.1+cu116)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369398 sha256=c76377aea6bb1084edf461a74bc7f38a0cc3011e4fb8ab538fcd1566a12dc9a7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bacnu4gp/wheels/c8/e4/e1/11374c111387672fc2068dfbe0d4b424cb9cdd1b2e184a71b5\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0-CDG12wPch",
        "outputId": "9f9d82d5-b2a4-4dc3-a56e-5806becbd9d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:03<00:00, 96.4MiB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 2.08MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 1.32MB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 172kB/s]\n",
            "Downloading pytorch_model.bin: 100% 501M/501M [00:01<00:00, 324MB/s]\n",
            "2023-04-01 01:06:37.973135: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-01 01:06:42.179599: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "train set size: 100001\n",
            "dev set size: 18335\n",
            "Train 0:   0% 0/1563 [00:00<?, ?it/s]/content/NewsClip/model.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  index1 = torch.tensor(index1, dtype=torch.int64)\n",
            "Train 0:  91% 1426/1563 [8:24:47<47:32, 20.82s/it]"
          ]
        }
      ],
      "source": [
        "!python main.py --ann_path=\"../drive/MyDrive/Colab_Notebooks/\" --gts_file_dev=\"../drive/MyDrive/Colab_Notebooks/val_gts.json\" --num_workers=0 --image_dir=\"./images_processed/\" --batch_size=64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzVbFRpOwQRm",
        "outputId": "9ef7f90f-871d-4fe0-ffbf-c078d534438e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-01 11:41:57.803112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-01 11:42:00.351853: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "train set size: 50001\n",
            "dev set size: 18335\n",
            "Train 0:   0% 0/782 [00:00<?, ?it/s]/content/NewsClip/model.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  index1 = torch.tensor(index1, dtype=torch.int64)\n",
            "Train 0: 100% 782/782 [5:43:53<00:00, 26.39s/it]\n",
            "Epoch [0/150], Loss: 10.8251, Perplexity: 50264.4569\n",
            "true==============\n",
            "Train 1:   2% 19/782 [08:33<5:43:32, 27.01s/it]Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1178, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 628, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 671, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/content/NewsClip/dataloader.py\", line 49, in __getitem__\n",
            "    article_embedding = text_model(**article)[\"last_hidden_state\"].detach()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 852, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 527, in forward\n",
            "    layer_outputs = layer_module(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 411, in forward\n",
            "    self_attention_outputs = self.attention(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 338, in forward\n",
            "    self_outputs = self.self(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 217, in forward\n",
            "    key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/NewsClip/main.py\", line 277, in <module>\n",
            "    main(args)\n",
            "  File \"/content/NewsClip/main.py\", line 126, in main\n",
            "    train(model=model,\n",
            "  File \"/content/NewsClip/main.py\", line 172, in train\n",
            "    for i, (imgs, caps_ids, caps_mask, caps_emb, caplens, img_ids, arts_ids, arts_mask, arts_emb, artslens) in enumerate(t):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1193, in __iter__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1299, in close\n",
            "    self.display(pos=0)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1492, in display\n",
            "    self.sp(self.__str__() if msg is None else msg)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 347, in print_status\n",
            "    fp_write('\\r' + s + (' ' * max(last_len[0] - len_s, 0)))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 340, in fp_write\n",
            "    fp.write(str(s))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/utils.py\", line 127, in inner\n",
            "    return func(*args, **kwargs)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python main.py --ann_path=\"../drive/MyDrive/Colab_Notebooks/\" --gts_file_dev=\"../drive/MyDrive/Colab_Notebooks/val_gts.json\" --num_workers=0 --image_dir=\"./images_processed/\" --batch_size=64 --checkpoint=\"./checkpoint/checkpoint_ClipNews_GoodNews.pth.tar\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFn82s6z3pcR"
      },
      "outputs": [],
      "source": [
        "!cp -r ./checkpoint/ ../drive/MyDrive/Colab_Notebooks/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls checkpoint/"
      ],
      "metadata": {
        "id": "hwp8irjm47kV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1853e77c-435b-4435-d1b0-e98fec256aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEST_model.pth.tar  checkpoint_ClipNews_GoodNews.pth.tar  README.MD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --ann_path=\"../drive/MyDrive/Colab_Notebooks/\" --gts_file_dev=\"../drive/MyDrive/Colab_Notebooks/val_gts.json\" --num_workers=0 --image_dir=\"./images_processed/\" --batch_size=64 --checkpoint=\"./checkpoint/checkpoint_ClipNews_GoodNews.pth.tar\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQniseBr7_bx",
        "outputId": "e91a7790-7086-470a-dd5b-8bb4180fcd55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-01 17:56:24.450558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-01 17:56:26.816572: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "train set size: 50001\n",
            "dev set size: 18335\n",
            "Train 1:   0% 0/782 [00:00<?, ?it/s]/content/NewsClip/model.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  index1 = torch.tensor(index1, dtype=torch.int64)\n",
            "Train 1: 100% 782/782 [5:58:44<00:00, 27.53s/it]\n",
            "Epoch [1/150], Loss: 10.8251, Perplexity: 50264.4181\n",
            "true==============\n",
            "Train 2:   1% 9/782 [04:16<6:07:14, 28.51s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/NewsClip/main.py\", line 277, in <module>\n",
            "    main(args)\n",
            "  File \"/content/NewsClip/main.py\", line 126, in main\n",
            "    train(model=model,\n",
            "  File \"/content/NewsClip/main.py\", line 172, in train\n",
            "    for i, (imgs, caps_ids, caps_mask, caps_emb, caplens, img_ids, arts_ids, arts_mask, arts_emb, artslens) in enumerate(t):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1178, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 628, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 671, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/content/NewsClip/dataloader.py\", line 49, in __getitem__\n",
            "    article_embedding = text_model(**article)[\"last_hidden_state\"].detach()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 852, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 527, in forward\n",
            "    layer_outputs = layer_module(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 453, in forward\n",
            "    layer_output = apply_chunking_to_forward(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pytorch_utils.py\", line 248, in apply_chunking_to_forward\n",
            "    return forward_fn(*input_tensors)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 466, in feed_forward_chunk\n",
            "    layer_output = self.output(intermediate_output, attention_output)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 377, in forward\n",
            "    hidden_states = self.dense(hidden_states)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r  ../drive/MyDrive/Colab_Notebooks/checkpoint/ ./checkpoint/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew9kIBPO8oNX",
        "outputId": "96e13286-5d63-46de-df27-64eb94f6f546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '../drive/MyDrive/Colab_Notebooks/checkpoint/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --ann_path=\"../drive/MyDrive/Colab_Notebooks/\" --gts_file_dev=\"../drive/MyDrive/Colab_Notebooks/val_gts.json\" --num_workers=0 --image_dir=\"./images_processed/\" --batch_size=32 --data_name=test"
      ],
      "metadata": {
        "id": "o-hL5cBf8_Lb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dff0ede4-30f4-4bee-c844-e0d2f0d81eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-02 11:02:05.908999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-02 11:02:08.619820: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "train set size: 23051\n",
            "dev set size: 18335\n",
            "Train 0:   0% 0/721 [00:00<?, ?it/s]/content/NewsClip/model.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  index1 = torch.tensor(index1, dtype=torch.int64)\n",
            "Train 0: 100% 721/721 [2:06:23<00:00, 10.52s/it]\n",
            "Epoch [0/150], Loss: 10.8250, Perplexity: 50264.2370\n",
            "true==============\n",
            "Train 1:  40% 287/721 [51:01<1:17:09, 10.67s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/NewsClip/main.py\", line 277, in <module>\n",
            "    main(args)\n",
            "  File \"/content/NewsClip/main.py\", line 126, in main\n",
            "    train(model=model,\n",
            "  File \"/content/NewsClip/main.py\", line 205, in train\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\", line 488, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 197, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.23 GiB (GPU 0; 14.75 GiB total capacity; 10.73 GiB already allocated; 1.18 GiB free; 12.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --ann_path=\"../drive/MyDrive/Colab_Notebooks/\" --gts_file_dev=\"../drive/MyDrive/Colab_Notebooks/val_gts.json\" --num_workers=0 --image_dir=\"./images_processed/\" --batch_size=32 --checkpoint=\"./checkpoint/checkpoint_test.pth.tar\""
      ],
      "metadata": {
        "id": "XdssQdw4W-Vw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3aa1aa-fe17-4758-c413-dfe6d2886c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-02 15:26:16.275471: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-02 15:26:18.580557: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "train set size: 23051\n",
            "dev set size: 18335\n",
            "Train 1:   0% 0/721 [00:00<?, ?it/s]/content/NewsClip/model.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  index1 = torch.tensor(index1, dtype=torch.int64)\n",
            "Train 1:  23% 168/721 [30:50<1:41:29, 11.01s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/NewsClip/main.py\", line 277, in <module>\n",
            "    main(args)\n",
            "  File \"/content/NewsClip/main.py\", line 126, in main\n",
            "    train(model=model,\n",
            "  File \"/content/NewsClip/main.py\", line 172, in train\n",
            "    for i, (imgs, caps_ids, caps_mask, caps_emb, caplens, img_ids, arts_ids, arts_mask, arts_emb, artslens) in enumerate(t):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1178, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 628, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 671, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/content/NewsClip/dataloader.py\", line 40, in __getitem__\n",
            "    caption_embedding = text_model(**caption)[\"last_hidden_state\"].detach()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 852, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 527, in forward\n",
            "    layer_outputs = layer_module(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 453, in forward\n",
            "    layer_output = apply_chunking_to_forward(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pytorch_utils.py\", line 248, in apply_chunking_to_forward\n",
            "    return forward_fn(*input_tensors)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 465, in feed_forward_chunk\n",
            "    intermediate_output = self.intermediate(attention_output)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 363, in forward\n",
            "    hidden_states = self.dense(hidden_states)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --ann_path=\"../drive/MyDrive/Colab_Notebooks/\" --gts_file_dev=\"../drive/MyDrive/Colab_Notebooks/val_gts.json\" --num_workers=0 --image_dir=\"./images_processed/\" --batch_size=64 --data_name=test_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u-FeAqvkzzf",
        "outputId": "ac0c7849-ee7e-4a2b-f95d-ef7f26145e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-02 16:06:00.464548: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-02 16:06:03.052633: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "train set size: 18596\n",
            "dev set size: 18335\n",
            "Train 0:   0% 0/291 [00:00<?, ?it/s]/content/NewsClip/model.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  index1 = torch.tensor(index1, dtype=torch.int64)\n",
            "Train 0: 100% 291/291 [1:44:45<00:00, 21.60s/it]\n",
            "Epoch [0/150], Loss: 10.8251, Perplexity: 50264.3777\n",
            "true==============\n",
            "Train 1: 100% 291/291 [1:40:29<00:00, 20.72s/it]\n",
            "Epoch [1/150], Loss: 10.8250, Perplexity: 50264.2766\n",
            "true==============\n",
            "Train 2: 100% 291/291 [1:39:18<00:00, 20.48s/it]\n",
            "Epoch [2/150], Loss: 10.8250, Perplexity: 50264.2779\n",
            "true==============\n",
            "Train 3:   3% 8/291 [03:00<1:46:42, 22.62s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/NewsClip/main.py\", line 277, in <module>\n",
            "    main(args)\n",
            "  File \"/content/NewsClip/main.py\", line 126, in main\n",
            "    train(model=model,\n",
            "  File \"/content/NewsClip/main.py\", line 172, in train\n",
            "    for i, (imgs, caps_ids, caps_mask, caps_emb, caplens, img_ids, arts_ids, arts_mask, arts_emb, artslens) in enumerate(t):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1178, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 628, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 671, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/content/NewsClip/dataloader.py\", line 49, in __getitem__\n",
            "    article_embedding = text_model(**article)[\"last_hidden_state\"].detach()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 852, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 527, in forward\n",
            "    layer_outputs = layer_module(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 453, in forward\n",
            "    layer_output = apply_chunking_to_forward(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pytorch_utils.py\", line 248, in apply_chunking_to_forward\n",
            "    return forward_fn(*input_tensors)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 466, in feed_forward_chunk\n",
            "    layer_output = self.output(intermediate_output, attention_output)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 377, in forward\n",
            "    hidden_states = self.dense(hidden_states)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --ann_path=\"../drive/MyDrive/Colab_Notebooks/\" --gts_file_dev=\"../drive/MyDrive/Colab_Notebooks/test_gts.json\" --num_workers=0 --image_dir=\"./images_processed/\" --batch_size=64 --data_name=test_3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8wUIa1HsRYl",
        "outputId": "8dbe9141-9916-4364-ebf1-99f80da6642d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-02 21:19:08.521429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-02 21:19:10.991092: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "train set size: 10870\n",
            "dev set size: 23051\n",
            "Train 0:   0% 0/170 [00:00<?, ?it/s]/content/NewsClip/model.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  index1 = torch.tensor(index1, dtype=torch.int64)\n",
            "Train 0: 100% 170/170 [57:44<00:00, 20.38s/it]\n",
            "Epoch [0/150], Loss: 10.8251, Perplexity: 50264.5322\n",
            "true==============\n",
            "Train 1: 100% 170/170 [57:58<00:00, 20.46s/it]\n",
            "Epoch [1/150], Loss: 10.8251, Perplexity: 50264.4024\n",
            "true==============\n",
            "Train 2:   4% 7/170 [02:36<1:00:44, 22.36s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/NewsClip/main.py\", line 277, in <module>\n",
            "    main(args)\n",
            "  File \"/content/NewsClip/main.py\", line 126, in main\n",
            "    train(model=model,\n",
            "  File \"/content/NewsClip/main.py\", line 172, in train\n",
            "    for i, (imgs, caps_ids, caps_mask, caps_emb, caplens, img_ids, arts_ids, arts_mask, arts_emb, artslens) in enumerate(t):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tqdm/std.py\", line 1178, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 628, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 671, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/content/NewsClip/dataloader.py\", line 49, in __getitem__\n",
            "    article_embedding = text_model(**article)[\"last_hidden_state\"].detach()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 852, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 527, in forward\n",
            "    layer_outputs = layer_module(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 453, in forward\n",
            "    layer_output = apply_chunking_to_forward(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/pytorch_utils.py\", line 248, in apply_chunking_to_forward\n",
            "    return forward_fn(*input_tensors)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 466, in feed_forward_chunk\n",
            "    layer_output = self.output(intermediate_output, attention_output)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 377, in forward\n",
            "    hidden_states = self.dense(hidden_states)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --ann_path=\"../drive/MyDrive/Colab_Notebooks/\" --gts_file_dev=\"../drive/MyDrive/Colab_Notebooks/test_gts.json\" --image_dir=\"./images_processed/\" --batch_size=64 --data_name=train_s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVvpvS281Ge9",
        "outputId": "a196e783-1967-40c7-c8e2-20d4d3b0fcfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-02 23:20:57.922027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-02 23:21:00.463180: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "train set size: 101\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "dev set size: 23051\n",
            "Train 0:   0% 0/2 [00:00<?, ?it/s]/content/NewsClip/model.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  index1 = torch.tensor(index1, dtype=torch.int64)\n",
            "Train 0: 100% 2/2 [00:36<00:00, 18.24s/it]\n",
            "Epoch [0/150], Loss: 10.8251, Perplexity: 50265.4091\n",
            "true==============\n",
            "Train 1: 100% 2/2 [00:34<00:00, 17.47s/it]\n",
            "Epoch [1/150], Loss: 10.8251, Perplexity: 50265.3175\n",
            "true==============\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/NewsClip/main.py\", line 277, in <module>\n",
            "    main(args)\n",
            "  File \"/content/NewsClip/main.py\", line 158, in main\n",
            "    save_checkpoint(args.data_name, epoch, args.epochs_since_improvement,\n",
            "  File \"/content/NewsClip/utils.py\", line 92, in save_checkpoint\n",
            "    torch.save(state, './checkpoint/BEST_model.pth.tar')\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 423, in save\n",
            "    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 650, in _save\n",
            "    zip_file.write_record(name, storage.data_ptr(), num_bytes)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1QxQt1-fFF7VijTa3QECbfK9DjHnePHE4",
      "authorship_tag": "ABX9TyPZe8fcvp7jgttlGWN555fP",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}